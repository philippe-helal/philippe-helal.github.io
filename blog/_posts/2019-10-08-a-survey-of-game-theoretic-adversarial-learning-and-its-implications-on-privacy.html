---
excerpt: |
    Adversarial learning is a new and growing area of machine-learning
    research. Formulating it using tools from game theory allows for a
    different view of machine learning, when compared to the traditional,
    purely statistical view. This view allows us to extend the scope of
    machine learning to security systems and privacy protection through the
    study of adversarial attacks, which allows for dealing with violations
    of the i.i.d assumption of machine learning that occur in some contexts.
    However, adversarial learning also raises some questions about the state
    and the future of machine learning. This survey provides a look at the
    intersection of machine learning and game theory by introducing the
    latter, then showing its application in adversarial learning. The survey
    continues with an overview of adversarial attacks and their known
    defenses, as well as prevention mechanisms for cases where sensitive
    data must not run the risk of being leaked by an adversarial attack.
author:
- Philippe Helal
bibliography:
- 'sample-base.bib'
key: 'a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy'
mathjax: True
mathjax_autoNumber: True
tags: 'paper survey game-theory machine-learning'
title: |
    A Survey of Game-Theoretic Adversarial Learning and Its Implications on
    Privacy
---
<em>**Note: This is a first draft of the survey, which is being submitted for review. Suggestions are most welcome.**</em>
<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Machine learning has become an essential part of human-machine interactions in one’s everyday life, sometimes against one’s knowledge or wish. From video-editing to voice assistants to autonomous vehicles, its benefits span a wide range of domains. Despite its seeming ubiquity, machine learning is a fast-growing research area that still faces a plethora of challenges, whether related to computational complexity <span class="citation" data-cites="Hestness:2019:BHA:3293883.3295710">[<a href="#ref-Hestness:2019:BHA:3293883.3295710" role="doc-biblioref">17</a>]</span> or data collection, among others. Among those challenges lies the rising threat of adversarial attacks <span class="citation" data-cites="2017arXiv170202284H">[<a href="#ref-2017arXiv170202284H" role="doc-biblioref">22</a>]</span>, in which adversaries employ methods designed to take advantage of a model’s flaws for their own gain. These methods vary according to the attacker’s access to training and test data, the type of model being considered, as well as the attacker’s goal. Such attacks are particularly present in security applications like antivirus or intrusion-detection systems, but can also be found in image recognition models. Such situations can be modeled as games in which 2 or more opponents interact, with each player looking to maximize his reward while accounting for his counterpart’s actions. Adding a game-theoretic approach to the statistical modeling inherent to machine learning allows for reinforcing a model by anticipating dangerous errors, thus reducing over-fitting. This arms race between model and attacker brings an increase in security, which might come at the cost of end-user privacy, whose attempts at preserving anonymity or pseudonymity is threatened by a model’s ability to detect them.</p>
<h1 id="game-theory-and-its-applications"><span class="header-section-number">2</span> Game theory and its applications</h1>
<h2 id="game-theory"><span class="header-section-number">2.1</span> Game theory</h2>
<p>Game theory is the study of mathematical models of strategic interaction between rational decision-makers <span class="citation" data-cites="myerson1997game">[<a href="#ref-myerson1997game" role="doc-biblioref">38</a>]</span>. Its growth as a discipline is attributed to von Neumann’s proof of the existence of mixed-strategy equilibria in two-person zero-sum games <span class="citation" data-cites="neumann1928theorie">[<a href="#ref-neumann1928theorie" role="doc-biblioref">41</a>]</span>. The theory has since been extended to n-player, cooperative or non-cooperative, zero-sum or non-zero-sum games. A fully-defined game is one that defines the following elements:</p>
<ul>
<li><p>The players</p></li>
<li><p>The information available to each player</p></li>
<li><p>The actions available to each player</p></li>
<li><p>The payoffs for each outcome</p></li>
</ul>
<p>Games are represented by decision trees (extensive form), matrices (normal form) or characteristic functions. Nash <span class="citation" data-cites="nash1951non">[<a href="#ref-nash1951non" role="doc-biblioref">40</a>]</span> proved that any finite n-player, non-zero-sum, non-cooperative game has at least one equilibrium point. A Nash equilibrium is a point in the space of all possible scenarios defined by the game, where each player performs the action that maximizes his own reward according to his prediction of the action(s) of the other player(s). In such a situation, no player has any incentive to change his strategy assuming all other players are constant in their strategies. The prisoner’s dilemma (Figure <a href="#fig:prisoner" data-reference-type="ref" data-reference="fig:prisoner">1</a>) is a well-known two-player zero-sum, normal form game, in which two individuals are being questioned separately for a crime they are suspected of. Assuming rationality of the players, the Nash equilibrium of prisoner’s dilemma is that they will both confess, as that is the winning strategy for each player given the other’s choice, and that will not change once a player is told the other’s strategy. Notice a Nash equilibrium is optimal for each player’s individual utility: it does not necessarily maximize total social welfare.</p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/Prisoners-dilemma.jpg" id="fig:prisoner" alt="" /><figcaption>Example game of prisoner’s dilemma. (<a href="https://bit.ly/2nRhqoa">https://bit.ly/2nRhqoa</a>)<span label="fig:prisoner"></span></figcaption>
</figure>
<h2 id="game-theory-in-computing"><span class="header-section-number">2.2</span> Game theory in computing</h2>
<p>Game theory and computer science interact in many scenarios. One the one hand, representing complex multiplayer games as well as analyzing the complexity of finding their Nash equilibria can be done through the lens of computer science: Kearn <span class="citation" data-cites="kearns2008graphical">[<a href="#ref-kearns2008graphical" role="doc-biblioref">25</a>]</span> provides an overview of graphical games and algorithms for finding their equilibria; Conitzer and Sandholm <span class="citation" data-cites="2002cs........5074C">[<a href="#ref-2002cs........5074C" role="doc-biblioref">7</a>]</span> demonstrate the algorithmic hardness of showing the existence, and the number, of a game’s Nash equilibria.</p>
<p>On the other hand, game theory can be a tool for finding optimal solutions to many computing problems: Koutsoupias and Papadimitriou <span class="citation" data-cites="Koutsoupias:1999:WE:1764891.1764944">[<a href="#ref-Koutsoupias:1999:WE:1764891.1764944" role="doc-biblioref">27</a>]</span> consider a noncooperative system in which players share a common resource. They propose a way of computing the cost of using the worst (in terms of social welfare) Nash equilibrium to the problem, where each player acts in his own self interest, to a socially-beneficial centralized solution. The advent of the Internet has enabled many applications that require interaction between agents. The cooperative operation of the Bitcoin <span class="citation" data-cites="nakamoto2008bitcoin">[<a href="#ref-nakamoto2008bitcoin" role="doc-biblioref">39</a>]</span> blockchain by miners around the world is vulnerable to attacks such as the 51% attack, where miners controlling the majority of the computing power could alter the mining process in their own selfish interest. Analysis of the decision each miner makes in cooperating or not with the system can be found in <span class="citation" data-cites="luu2015power">[<a href="#ref-luu2015power" role="doc-biblioref">35</a>]</span>, <span class="citation" data-cites="eyal2015miner">[<a href="#ref-eyal2015miner" role="doc-biblioref">11</a>]</span>, <span class="citation" data-cites="johnson2014game">[<a href="#ref-johnson2014game" role="doc-biblioref">23</a>]</span>.</p>
<h1 id="s:adversarial-training"><span class="header-section-number">3</span> Adversarial learning</h1>
<p>Adversarial learning consists of hardening a machine-learning model by presenting it with with inputs crafted to fool it. Traditional machine learning consists of an optimization problem, defined by a discriminative function <span class="math inline">\(f\)</span>, parameterized by a set <span class="math inline">\(\theta\)</span>, and trained to optimize a loss function <span class="math inline">\(L\)</span> on <span class="math inline">\(n \times d\)</span>-dimensional input data <span class="math inline">\(X\)</span>. The optimization algorithm’s goal is to find the set of parameters <span class="math inline">\(\theta^*\)</span> that minimizes the loss with respect to the training data. Equation 1 shows a general formulation of this objective. <span class="math display">\[\label{eq:mlobjective}
 \theta^* = \mathop{\mathrm{arg\,min}}_\theta \frac{1}{n} \sum_{i=1}^n L(f_\theta(x_i), x_i)\]</span></p>
<p>Once <span class="math inline">\(\theta^*\)</span> is solved for, <span class="math inline">\(f(x)\)</span> is measured on some test data that is separate from the training dataset. Underlying the optimization procedure is the assumption that all inputs from both the training and testing datasets are sampled independently from the same distribution. In other words, it assumes that all data points come from the same, constant, data-generating process. While the assumption holds true in many training scenarios, real-life data may show it to be overly simplistic. In many practical applications, failing to account for slightly out-of-distribution inputs can have far-reaching implications. An attacker can take advantage of this assumption in two different ways.</p>
<p>First, when the training data is gathered from uncontrolled sources, such as public forum posts or online images, the adversary can deliberately insert malicious examples into these sources to be picked up by the algorithm as coming from the same distribution as the rest of the data. The larger the dataset is, the more difficult it is for the modeler to notice adversarial examples.</p>
<p>Second, malicious examples can be fed to the model at test time to closely resemble those the model has already seen while being altered specifically to fool the algorithm, leading them to be misclassified. Cybersecurity applications, for example antivirus solutions, spam filtering and intrusion-detection systems <span class="citation" data-cites="SHEN20143">[<a href="#ref-SHEN20143" role="doc-biblioref">51</a>]</span>, benefit particularly from adversarial learning: these systems are under constant threat of a well-crafted attack going undetected, with potentially dire consequences. For example, spyware can be hidden into innocent-looking browser toolbars, malware can be spread through peer-to-peer sharing services, and information can be stolen via malicious links in e-mail. In such a situation, the security system and its attackers are in constant learning states, where the attackers attempt to learn the detection algorithm employed by the system (in order to fool it), and the system continually improves by attempting to detect novel attacks and integrating them into the detection process.</p>
<p>An important step in hardening the algorithm is modeling the attacker. Huang <span><em>et al.</em></span> <span class="citation" data-cites="Huang:2011:AML:2046684.2046692">[<a href="#ref-Huang:2011:AML:2046684.2046692" role="doc-biblioref">21</a>]</span> decompose it into three facets: knowledge, goals and capabilities. Each factor is presented in the following subsections.</p>
<h2 id="adversarial-knowledge"><span class="header-section-number">3.1</span> Adversarial knowledge</h2>
<p>The attacker’s knowledge of the system will influence the design of the attacks. Three main components are identified: the algorithm, its feature space, and the training and testing datasets.</p>
<h3 id="knowledge-of-the-algorithm"><span class="header-section-number">3.1.1</span> Knowledge of the algorithm</h3>
<p>The attacker will hardly have perfect knowledge of the learning algorithm, unless it is publicly known or published. He can, however, anticipate the model class being used.</p>
<h3 id="knowledge-of-the-feature-space"><span class="header-section-number">3.1.2</span> Knowledge of the feature space</h3>
<p>Feature engineering and dimensionality reduction techniques will generally be partially or fully unknown by the attacker. Nonetheless, some domain-specific features can be common to multiple algorithms, and successful attack of one learner can lead to a better capacity of inferring some parts of the feature space.</p>
<h3 id="knowledge-of-the-data"><span class="header-section-number">3.1.3</span> Knowledge of the data</h3>
<p>The adversary’s knowledge of the training and testing data will depend on his relationship with the target. Insiders potentially have full access to it, while foreign adversaries can get information about it. For example, <a href="https://virustotal.com">https://virustotal.com</a> acts as an online database of files scanned by the most popular antivirus solutions, as well as the classification made by each.</p>
<h2 id="adversarial-capabilities"><span class="header-section-number">3.2</span> Adversarial capabilities</h2>
<p>An attacker’s capabilities is closely linked to his knowledge of the target. For example, inside access to the data gives the attacker the power to alter it. Two types of influence are identified.</p>
<h3 id="causative-influence"><span class="header-section-number">3.2.1</span> Causative influence</h3>
<p>The attacker has the ability to alter the training data and affect the classification of specific inputs.</p>
<h3 id="exploratory-influence"><span class="header-section-number">3.2.2</span> Exploratory influence</h3>
<p>The attacker cannot alter the algorithm’s training data, but can test the system in order to gather information about its data.</p>
<h2 id="adversarial-goals"><span class="header-section-number">3.3</span> Adversarial goals</h2>
<p>The attack will be defined by the adversary’s final objective, which depends largely on the type of target and the information that it could expose. This objective is composed of the security violation desired and specificity of the attack.</p>
<h3 id="security-violation"><span class="header-section-number">3.3.1</span> Security violation</h3>
<p>The attacker can target the <strong>integrity</strong> of the application, leading to false negatives being output by the classifier, or its <strong>availability</strong>, leading to false negatives <span><em>and</em></span> false positives and rendering the classifier unusable. He can alternatively target the application’s <strong>privacy</strong> by learning some information from the system and potentially gathering private data about its users.</p>
<h3 id="specificity"><span class="header-section-number">3.3.2</span> Specificity</h3>
<p>A <strong>targeted</strong> attack will be aimed at specific data points (<span><em>e.g.</em></span> classifying malicious popups as safe), while an <strong>indiscriminate</strong> one will have a broader objective (<span><em>e.g.</em></span> misclassifying items of any class).</p>
<h1 id="adversarial-learning-as-a-game"><span class="header-section-number">4</span> Adversarial learning as a game</h1>
<p>In a game of adversarial learning, the players are well defined: the learner represents one side of the game, while its attacker(s) represent the opposing side. The information available to each can be modeled using the characteristics described in section <a href="#s:adversarial-training" data-reference-type="ref" data-reference="s:adversarial-training">3</a>. What remains is to define the actions they may take as well as their payoffs. In order to do this, we need to define the playing situation. This section presents two such situations: sequential games and simultaneous games.</p>
<h2 id="sequential-game"><span class="header-section-number">4.1</span> Sequential game</h2>
<p>In a sequential or Stackelberg game, a <strong>leader</strong> starts by making the first move. His <strong>follower</strong> then makes his move, with full knowledge of the leader’s strategy. In such a game, each player has an advantage. The follower’s is obvious: he can observe the leader’s move and decide on his own best strategy. In other words, he does not need to model the game as a function of the leader’s action: the leader has already acted and the follower’s possible moves and their payoffs are known. The leader also benefits: by acting first, he effectively dictates the game to his opponent, restraining his actions in order to maximize his own reward. As Stackelberg games are considered constant-sum games, this strategy leads to a minimization of the follower’s reward. Research has been done modeling adversarial learning as such games, with the learner either as the leader or the follower.</p>
<h3 id="learner-as-the-follower"><span class="header-section-number">4.1.1</span> Learner as the follower</h3>
<p>Liu and Chawla <span class="citation" data-cites="Liu2010">[<a href="#ref-Liu2010" role="doc-biblioref">34</a>]</span> propose a game where the attacker is the leader, and a classifier retroactively defends itself against an attack once it has been detected. They pose the following restriction: the classifier’s loss function must be convex and sub-differentiable: deep neural networks, whose optimization is often non-convex, are not considered valid in this setting. In this game, a binary classifier is tested by an adversary by receiving linearly-shifted inputs, which get misclassified by the learner. Upon detection of its error, the learner then learns to adjust its decision boundary to account for these new examples. Figure <a href="#fig:liu2010" data-reference-type="ref" data-reference="fig:liu2010">2</a> shows an example of a linear classifier playing one round of such a game. In their example, the payoff function of the adversary corresponds to the learner’s loss function, corresponding to the negative of the classifier’s payoff. In other words:</p>
<p><span class="math display">\[\label{eq:liu2010}
 J_A(\alpha) = -J_C(\theta) = L(f_\theta(x_i), x_i + \alpha)\]</span></p>
<p>where <span class="math inline">\(J\)</span> is a payoff function, <span class="math inline">\(\alpha\)</span> is the shift applied to the inputs by the adversary <span class="math inline">\(A\)</span>, and <span class="math inline">\(\theta\)</span> is the set of parameters of the classifier <span class="math inline">\(C\)</span>.</p>
<p>This leads to a zero-sum game with a Nash equilibrium where the learner’s weights and the adversary’s data manipulations have no incentive to change.</p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/liu2010.png" id="fig:liu2010" alt="" /><figcaption>Example sequential adversarial learning setting. (Liu and Chawla, 2010)<span label="fig:liu2010"></span></figcaption>
</figure>
<h3 id="learner-as-the-leader"><span class="header-section-number">4.1.2</span> Learner as the leader</h3>
<p>In some games, such as in cybersecurity systems, waiting for malicious inputs in order to adjust its parameters is too costly for the learner. In these cases, the learner is pre-trained on large amounts of data before being made available to the attacker. Its parameters are therefore fixed and available to the attacker through inference. The attacker then tries to learn these parameters by trying different input transformations and getting the learner’s classification. Spam filters are examples of such games. Brückner and Scheffer <span class="citation" data-cites="Bruckner:2011:SGA:2020408.2020495">[<a href="#ref-Bruckner:2011:SGA:2020408.2020495" role="doc-biblioref">5</a>]</span> formulate the optimization problem and its resulting solution as a Stackelberg prediction game using different classifier loss functions.</p>
<p>The games considered so far assume the adversary has but one way to transform inputs. It is also possible for an adversary to have multiple tools at his disposal: we model such a game as a multi-follower game. Zhou and Kantarcioglu <span class="citation" data-cites="10.1007/978-3-319-31750-2_28">[<a href="#ref-10.1007/978-3-319-31750-2_28" role="doc-biblioref">56</a>]</span> present a game in which a learner is opposed to multiple independent adversaries, with data transformations of varying types and degrees. They propose a framework consisting of many different Stackelberg games, each of which places a predictive model against one of the adversaries. The optimal equilibrium strategies for each of these games are used by a Bayesian Stackelberg game in which each of their classifiers is assigned a probability of being used by the learner. The added randomness makes it more difficult for the adversaries to infer the classifier’s parameters: they must now first infer the classifier chosen, and then learn its parameters.</p>
<h2 id="simultaneous-game"><span class="header-section-number">4.2</span> Simultaneous game</h2>
<p>In simultaneous games such as the prisoner’s dilemma (Figure <a href="#fig:prisoner" data-reference-type="ref" data-reference="fig:prisoner">1</a>), players must pick their strategy without knowing what their opponent have chosen. They simultaneously pick the action that optimizes their payoff as a function of the other player’s action. Extensive research has been done on this topic, particularly in the case of adversary feature modification or deletion. In such an attack, the adversary removes or alters features that were available during training from test inputs (by setting them to zero).</p>
<p>Brückner and Scheffer <span class="citation" data-cites="NIPS2009_3755">[<a href="#ref-NIPS2009_3755" role="doc-biblioref">4</a>]</span> explore simultaneous non-zero-sum games to identify conditions for determining the existence of a (unique) Nash equilibrium, which then allows to find the classifier corresponding to the equilibrium state.</p>
<p>Dekel, Shamir and Xiao <span class="citation" data-cites="Dekel2010">[<a href="#ref-Dekel2010" role="doc-biblioref">8</a>]</span> analyze the learning procedure of a binary classifier exposed to such a test-time threat. The game is formulated first as a linear programming exercise, and then as training of a modified Perceptron <span class="citation" data-cites="rosenblatt1958perceptron">[<a href="#ref-rosenblatt1958perceptron" role="doc-biblioref">48</a>]</span>. Globerson and Rowels <span class="citation" data-cites="Globerson:2006:NTT:1143844.1143889">[<a href="#ref-Globerson:2006:NTT:1143844.1143889" role="doc-biblioref">14</a>]</span> show, through quadratic programming, how to construct classifiers such as support vector machines that are robust to feature deletion.</p>
<p>Zhou <span><em>et al.</em></span> <span class="citation" data-cites="Zhou:2012:ASV:2339530.2339697">[<a href="#ref-Zhou:2012:ASV:2339530.2339697" role="doc-biblioref">57</a>]</span> present different optimal learner strategies based on attack models. In a <strong>free-range</strong> attack, the adversary could make any modification to any dimension of the input data. This assumes complete power of the adversary to test the learner on transformations of all kinds. In a <strong>restrained</strong> attack, the adversary is predicted to be rational, meaning his possible actions are limited to those that increase his payoff. Such transformations are generally small, with the goal of transforming an input just enough to fool the classifier. This restriction reduces the number of false positives reported by the classifier on real-life datasets in comparison to the free-range attack.</p>
<h2 id="gans"><span class="header-section-number">4.3</span> GANs</h2>
<p>Generative adversarial networks <span class="citation" data-cites="NIPS2014_5423">[<a href="#ref-NIPS2014_5423" role="doc-biblioref">16</a>]</span> are today’s most prominent example of game-theoretic learning. Such models are in fact a pair of opposed models: a <strong>discriminator</strong> <span class="math inline">\(D\)</span>, corresponding to a binary classifier; and a <strong>generator</strong> <span class="math inline">\(G\)</span>, consisting of an unsupervised neural network attempting to learn the distribution of inputs that the discriminator classifies as positive. The setting is as follows: starting with a dataset <span class="math inline">\(X\)</span> of inputs (typically images), <span class="math inline">\(G\)</span> samples random noise from a distribution <span class="math inline">\(z\)</span> to try to produce new inputs that are statistically similar to the ones from <span class="math inline">\(X\)</span>. Inputs are fed from either <span class="math inline">\(X\)</span> or <span class="math inline">\(G\)</span>, at random, to <span class="math inline">\(D\)</span>, which is tasked with predicting whether the input is real or generated. Both models are trained simultaneously: <span class="math inline">\(D\)</span> to learn to recognize real inputs; <span class="math inline">\(G\)</span> to learn to generate realistic inputs. Figure <a href="#fig:gan-architecture" data-reference-type="ref" data-reference="fig:gan-architecture">3</a> shows a visual representation of the training procedure.</p>
<p>Formally, GAN training consists of a zero-sum game with <span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span> as the player, and the following minimax objective: <span class="math display">\[\label{eq:gan-objective}
    \arg \min_{\theta^{(G)}} \max_{\theta^{(D)}} \mathbf{E}_{x \sim \mathbf{P}_r} [\log(D(x)] + \mathbf{E}_{z \sim \mathbf{p(z)}} [\log(1 - D(G(z)]\]</span></p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/gan-architecture.png" id="fig:gan-architecture" alt="" /><figcaption>GAN training procedure. (<a href="https://bit.ly/2M8xXOz">https://bit.ly/2M8xXOz</a>)<span label="fig:gan-architecture"></span></figcaption>
</figure>
<p>Training runs until <span class="math inline">\(D\)</span> is unable to distinguish <span class="math inline">\(G(z)\)</span> from <span class="math inline">\(X\)</span>. In other words, convergence is achieved when <span class="math inline">\(D\)</span> consistently makes predictions with 50% confidence. At that point, the algorithm has produced a generator capable of producing realistic inputs, as well as a strong classifier that can be reused in other scenarios.</p>
<p>GANs, as proposed in the original paper, are affected by a few problems. First, training is unstable and may never converge. In such a case, <span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span>’s payoffs are constantly oscillating without improving significantly and have to be re-initialized. Second, they are vulnerable to mode collapse, which is a state in which <span class="math inline">\(G\)</span> has found one or a few inputs that fool <span class="math inline">\(D\)</span>, and consistently produces inputs extremely similar to them, leading to very little variety in the generated examples. Third, measuring <span class="math inline">\(G\)</span>’s performance is very difficult: researchers often resort to manually inspecting the realism of the generated examples.</p>
<p>Since the publication of the original paper, GANs have been one of the most intensely-researched subject areas in the machine learning literature. Work has been done notably to improve the realism of data output by the generator, as well as to stabilize the training procedure.</p>
<h3 id="wasserstein-gan"><span class="header-section-number">4.3.1</span> Wasserstein GAN</h3>
<p>Arjovsky <span><em>et al.</em></span> <span class="citation" data-cites="2017arXiv170107875A">[<a href="#ref-2017arXiv170107875A" role="doc-biblioref">2</a>]</span> propose the use of the Wasserstein distance as a replacement for the KL-divergence in the objective function of the original GAN. This improvement lead to fewer mode collapses and more stable learning. Their paper also provides ways to measure the model’s performance as it’s being trained, for quicker optimization and debugging.</p>
<h3 id="deep-convolutional-gan"><span class="header-section-number">4.3.2</span> Deep Convolutional GAN</h3>
<p>Radford <span><em>et al.</em></span> <span class="citation" data-cites="2015arXiv151106434R">[<a href="#ref-2015arXiv151106434R" role="doc-biblioref">46</a>]</span> propose the use of convolutional neural networks <span class="citation" data-cites="lecun1995convolutional">[<a href="#ref-lecun1995convolutional" role="doc-biblioref">31</a>]</span> in the generator and discriminator for image generation tasks. CNNs are the standard in supervised image classification, but had not been previously used in unsupervised tasks such as image generation. In this paper, the authors show the superiority of convolutional generative adversarial networks in comparison to traditional feed-forward neural networks.</p>
<h3 id="applications-to-various-domains"><span class="header-section-number">4.3.3</span> Applications to various domains</h3>
<h4 id="image-to-image-translation"><span class="header-section-number">4.3.3.1</span> Image-to-image translation</h4>
<p>GANs have been improved and used in a number of contexts. Zhu <span><em>et al.</em></span> <span class="citation" data-cites="CycleGAN2017">[<a href="#ref-CycleGAN2017" role="doc-biblioref">58</a>]</span> have developed an adversarial approach to image-to-image translation, which consists of learning the relationship between two images from different domains (<span><em>e.g.</em></span> a photograph and a painting of the same subject). The novelty of their approach is that it does not need paired images in the training data in order to learn their relationship.</p>
<h4 id="image-upsampling"><span class="header-section-number">4.3.3.2</span> Image upsampling</h4>
<p>Most deep-learning algorithms trained for image-recognition or generation tasks work with relatively small images (typically 64x64 pixels). While the images are suitable from a learning perspective, one may need larger resolutions of the image for presentation purposes. Ledig <span><em>et al.</em></span> <span class="citation" data-cites="Ledig_2017_CVPR">[<a href="#ref-Ledig_2017_CVPR" role="doc-biblioref">32</a>]</span> propose a generative adversarial network capable of extracting the structure of an image in order to enlarge it by a factor of up to 4. They accomplish this by introducing a loss that is composed of an adversarial loss and a content loss. The adversarial loss allows the discriminator to test its ability to distinguish original images from the ones that were first upsampled by the generator. The content loss measures the loss of detail, such as high pixellation, that occurred after resizing the image.</p>
<h4 id="realistic-image-generation"><span class="header-section-number">4.3.3.3</span> Realistic image generation</h4>
<p>Unsurprisingly, much of the research in GANs has been made to push its limits and allow it to generate as realistic a picture as possible. Karras <span><em>et al.</em></span> <span class="citation" data-cites="2017arXiv171010196K">[<a href="#ref-2017arXiv171010196K" role="doc-biblioref">24</a>]</span> propose a GAN architecture allowing for style transfer of images, and able to generate highly-realistic faces. Some of its outputs are shown in Figure <a href="#fig:stylegan" data-reference-type="ref" data-reference="fig:stylegan">4</a>.</p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/stylegan.png" id="fig:stylegan" alt="" /><figcaption>StyleGAN-generated images.<span label="fig:stylegan"></span></figcaption>
</figure>
<p>Park <span><em>et al.</em></span> <span class="citation" data-cites="Park_2019_CVPR">[<a href="#ref-Park_2019_CVPR" role="doc-biblioref">45</a>]</span> have deployed a generative model capable of creating images from a semantic sketch delimiting the various elements to be drawn (<span><em>e.g.</em></span> trees, water, sky).</p>
<h1 id="adversarial-attacks-and-defenses"><span class="header-section-number">5</span> Adversarial attacks and defenses</h1>
<p>Adversarial attacks are another highly-researched application of adversarial learning. In this section, we present common attacks employed by adversaries depending on their level of knowledge of the learner and the tools at their disposal. We find three major types of attacks: evasion, poisoning, and exploratory attacks.</p>
<h2 id="evasion-attacks"><span class="header-section-number">5.1</span> Evasion attacks</h2>
<p>Evasion attacks consist of test-time generation of corrupted input data, crafted to be misclassified by the learner. In such attacks, the adversary has no ability to affect model training. Papernot <span><em>et al.</em></span> <span class="citation" data-cites="7546524">[<a href="#ref-7546524" role="doc-biblioref">44</a>]</span> formulate the adversary’s attack strategy as a two-step iterative method consisting of first finding the input dimensions that most affect a change in the classifier’s prediction, and second to evaluate the model based on an input transformed in these dimensions. The goal is to produce a transformation as small as possible to be misclassified by the model, while being imperceivable to a human observer. Common methods for performing the first step are presented here:</p>
<h3 id="ss:fgsm"><span class="header-section-number">5.1.1</span> Fast Gradient Sign Method</h3>
<p>Proposed by Goodfellow <span><em>et al.</em></span> <span class="citation" data-cites="2014arXiv1412.6572G">[<a href="#ref-2014arXiv1412.6572G" role="doc-biblioref">15</a>]</span>, this method alters the inputs by adding a perturbation equal to the sign of the gradient of the classifier’s loss function, scaled by a factor <span class="math inline">\(\epsilon\)</span>. In other words: <span class="math display">\[x = x + \epsilon \cdot \text{sign}\left(\nabla_X L(x, y)\right)\]</span> where <span class="math inline">\(y\)</span> is the true label assigned to <span class="math inline">\(x\)</span>. This can lead to a classifier predicting the wrong class with a high confidence, as shown in Figure <a href="#fig:fgsm" data-reference-type="ref" data-reference="fig:fgsm">5</a>.</p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/fgsm.png" id="fig:fgsm" alt="" /><figcaption>Comparison of original and FGSM-altered image (Goodfellow <span><em>et al.)</em></span><span label="fig:fgsm"></span></figcaption>
</figure>
<p>Two variants of FGSM have been suggested, both by Kurakin, Goodfellow and Bengio:</p>
<h4 id="target-class-method"><span class="header-section-number">5.1.1.1</span> Target class method</h4>
<p>Similar to the original FGSM, this method <span class="citation" data-cites="2016arXiv161101236K">[<a href="#ref-2016arXiv161101236K" role="doc-biblioref">29</a>]</span> however tries to fool the classifier into predicting a <span><em>specific</em></span> class <span class="math inline">\(y_t\)</span>. The optimization resembles the original FSGM, but replaces the true label <span class="math inline">\(y\)</span> with <span class="math inline">\(y_t\)</span> in the objective function.</p>
<h4 id="basic-iterative-method"><span class="header-section-number">5.1.1.2</span> Basic Iterative Method</h4>
<p>In this extension <span class="citation" data-cites="2016arXiv160702533K">[<a href="#ref-2016arXiv160702533K" role="doc-biblioref">28</a>]</span> of FGSM, the authors suggest to iteratively modify <span class="math inline">\(x\)</span> using element-wise clipping and show it to be able to fool smartphone cameras.</p>
<h3 id="deepfool"><span class="header-section-number">5.1.2</span> DeepFool</h3>
<p>Similar to the basic iterative method, DeepFool <span class="citation" data-cites="Moosavi-Dezfooli_2016_CVPR">[<a href="#ref-Moosavi-Dezfooli_2016_CVPR" role="doc-biblioref">37</a>]</span> gradually perturbs inputs in order to have the learner (a deep neural network) misclassify them. The algorithm modifies the original image by moving in the direction of the loss function’s gradient (rather than its sign, as in FGSM) by projecting the example onto the learner’s decision boundary, and gets the class prediction from the label. The algorithm runs until the predicted class changes.</p>
<h3 id="jacobian-saliency-map"><span class="header-section-number">5.1.3</span> Jacobian saliency map</h3>
<p>Papernot <span><em>et al.</em></span> <span class="citation" data-cites="7467366">[<a href="#ref-7467366" role="doc-biblioref">43</a>]</span> provide a more direct approach to finding the sensitivity of the classifier to input dimensions. They propose to calculate the Jacobian of the classifier’s objective function in order to obtain the gradients of each possible class with respect to the input’s dimensions. They then modify only the smallest subset of the input’s dimensions that allows for effectively fooling the learner.</p>
<h3 id="carlini-wagner-attack"><span class="header-section-number">5.1.4</span> Carlini-Wagner attack</h3>
<p>Carlini and Wagner <span class="citation" data-cites="7958570">[<a href="#ref-7958570" role="doc-biblioref">6</a>]</span> formulate the adversarial generation objective as trying to find the smallest possible change <span class="math inline">\(\delta\)</span> that will lead the learner to misclassify an example <span class="math inline">\(x\)</span> with high confidence. The objective is formalized as the following: <span class="math display">\[\delta^* = \mathop{\mathrm{arg\,min}}_\delta D(x, x + \delta) + c \cdot f(x + \delta), s.t. x + \delta \in [0, 1]^n\]</span> where <span class="math inline">\(D\)</span> is a distance metric between the original and perturbed examples, <span class="math inline">\(c\)</span> is a constant scaling factor and <span class="math inline">\(f\)</span> is an objective function such as the cross-entropy loss. They also constrain the search space of <span class="math inline">\(\delta\)</span> by forcing the algorithm to generate valid pixel values, <span><em>i.e.</em></span> from 0 to 1. They test their attack on the MNIST and CIFAR-10 datasets using each of the <span class="math inline">\(L_2, L_0\)</span> and <span class="math inline">\(L_\infty\)</span> distances for <span class="math inline">\(D\)</span> and 6 different objective functions for <span class="math inline">\(f\)</span>. Their attack method was able to achieve near-perfect (over 99%) misclassification on CIFAR-10, but the algorithm is computationally expensive.</p>
<h3 id="gan-based-attacks"><span class="header-section-number">5.1.5</span> GAN-based attacks</h3>
<p>The generator of GANs can also be used for evasion attacks. The game setting involves a pretrained black-box discriminator <span class="math inline">\(D\)</span>, which the generator <span class="math inline">\(G\)</span> will try to fool. GANs can be an effective adversarial tool when trained properly, and a simple pass through the generator network provides a more efficient generation process than the Carlini-Wagner attack. Bose and Aarabi <span class="citation" data-cites="8547128">[<a href="#ref-8547128" role="doc-biblioref">3</a>]</span> propose a white-box approach to generating adversarial examples for pretrained state-of-the-art Faster R-CNN <span class="citation" data-cites="NIPS2015_5638">[<a href="#ref-NIPS2015_5638" role="doc-biblioref">47</a>]</span> models. Their optimization function takes inspiration from the Carlini-Wagner formulation, and produces examples at a much higher rate. However, their method is limited to Faster R-CNN face detection classifiers, and therefore requires the adversary to know which learner class he is confronted to. Hu and Tan <span class="citation" data-cites="2017arXiv170205983H">[<a href="#ref-2017arXiv170205983H" role="doc-biblioref">20</a>]</span> apply GAN-based attacks to malware-detection systems. Their attack assumes the adversary has knowledge of the features used by the detection system (<span><em>e.g.</em></span> whether or not the example has the ability to write files to the computer).</p>
<h2 id="ss:gt-analysis"><span class="header-section-number">5.2</span> Game-theoretic analysis</h2>
<p>Oh <span><em>et al.</em></span> <span class="citation" data-cites="8237427">[<a href="#ref-8237427" role="doc-biblioref">42</a>]</span> model privacy protection as a game between a user <span class="math inline">\(U\)</span> and a recogniser <span class="math inline">\(R\)</span>. In adversarial-learning terms, the user can be viewed as the attacker, whereas the recogniser is the learner. Neither player knows their opponent’s chosen strategy, but they are aware of his strategy space: <span class="math inline">\(\Omega^U\)</span> and <span class="math inline">\(\Omega^R\)</span> for the user and recognizer, respectively. <span class="math inline">\(R\)</span>’s payoff is defined by <span class="math inline">\(p_{ij}\)</span>, corresponding to its accuracy rate for a tuble of strategies (<span class="math inline">\(i \in \Omega^U, j \in \Omega^R\)</span>), while <span class="math inline">\(U\)</span>’s payoff corresponds to <span class="math inline">\(R\)</span>’s misclassification rate, and is therefore equal to <span class="math inline">\(1 - p_{ij}\)</span>. We consider this a 2-player game with a constant sum of 1. Each player can use a mixed strategy, in which his chosen strategy will be sampled randomly from a distribution <span class="math inline">\(\theta\)</span> over his strategy space. <span class="math inline">\(U\)</span>’s optimal mixed strategy distribution can be found by optimising the following minimax objective: <span class="math display">\[\theta^* = \mathop{\mathrm{arg\,min}}_{\theta^U} \max_{\theta^O} \sum_{i,j} \theta^U \theta^O p_{ij}\]</span></p>
<p>Playing this optimal strategy then guarantees <span class="math inline">\(U\)</span> a certain level of privacy (corresponding to <span class="math inline">\(R\)</span>’s misclassification rate), regardless of <span class="math inline">\(R\)</span>’s action. The user will use additive evasion attacks such as FGSM (subsection <a href="#ss:fgsm" data-reference-type="ref" data-reference="ss:fgsm">5.1.1</a>) to accomplish as high as misclassification as he can. The authors apply their methodology to a face-recognition task on 4 highly-accurate CNN architectures with various Adversarial Image Perturbation Strategies (AIPs).</p>
<h2 id="poisoning-attacks"><span class="header-section-number">5.3</span> Poisoning attacks</h2>
<p>In poisoning attacks, the adversary slips infected inputs into the learner’s training data, while assigning them to a desired class. The classifier effectively learns to be misled by such examples. Those inputs can then be passed to the model at test time and should not be detected as malicious.</p>
<p>Such attacks can be particularly harmful in collaborative systems, such as collaborative filtering and recommender systems. Li <span><em>et al.</em></span> <span class="citation" data-cites="NIPS2016_6142">[<a href="#ref-NIPS2016_6142" role="doc-biblioref">33</a>]</span> present attacks to the availability and integrity of such a system, as well as a mixed strategy.</p>
<p>Anomaly detection systems are another vital example of binary classification tasks. Poisoning such a system has potentially serious security implications. Kloft <span><em>et al.</em></span> <span class="citation" data-cites="pmlr-v9-kloft10a">[<a href="#ref-pmlr-v9-kloft10a" role="doc-biblioref">26</a>]</span> solve for the optimal poisoning attack on an online centroid anomaly detection, and analyze its impact.</p>
<h3 id="black-box-attacks-through-transferability"><span class="header-section-number">5.3.1</span> Black-box attacks through transferability</h3>
<p>The methods presented above assumed some knowledge of the learner: either its model class, training distribution or parameter set (or a combination of those). However, in the many cases where no information is available about the learner, it is still possible to use those attacks by training a similar model on another dataset, and use the transferability property of machine learning models to attack the original target. Tramèr <span><em>et al.</em></span> <span class="citation" data-cites="2017arXiv170403453T">[<a href="#ref-2017arXiv170403453T" role="doc-biblioref">53</a>]</span> show that highly-dimensional adversarial examples share subspaces and that different models share similar directions in their decision boundaries, allowing adversarial inputs to have similar effects on multiple different models.</p>
<h2 id="exploratory-attacks"><span class="header-section-number">5.4</span> Exploratory attacks</h2>
<p>As their name implies, exploratory attacks are used by adversaries with an exploratory influence on the target. They consist of inferring the state of the learner by iteratively providing examples that the adversary thinks are likely to be misclassified.</p>
<h3 id="model-inversion"><span class="header-section-number">5.4.1</span> Model inversion</h3>
<p>Fredrikson <span><em>et al.</em></span> <span class="citation" data-cites="Fredrikson:2015:MIA:2810103.2813677">[<a href="#ref-Fredrikson:2015:MIA:2810103.2813677" role="doc-biblioref">12</a>]</span> develop model inversion attacks in which an attacker can query a black-box model for prediction on an example, and receive confidence values for that input. Their technique also allows for recovering the training data used by the learner with reasonable accuracy. For example, they are able to regenerate training images used by a facial recognition API, by knowing only a person’s name and querying the model for predictions. They test their technique using a softmax regression model, a stacked denoising autoencoder and a multi-layer perceptron.</p>
<h3 id="model-extraction"><span class="header-section-number">5.4.2</span> Model extraction</h3>
<p>Model extraction attacks, as introduced by Tramèr <span><em>et al.</em></span> <span class="citation" data-cites="197128">[<a href="#ref-197128" role="doc-biblioref">54</a>]</span>, consist of learning the black-box learner for which they only have query access to the predictions (and possibly its confidence values). In other words, their goal is to reproduce the model. This has repercussions particularly in machine-learning-as-a-service systems - where customers pay to get predictions on their data from a capable pre-trained model - as this would allow a successful adversary to resell the model as his own, or publish it for free. The authors test their attacks on two popular such services offered by Google and Amazon, for many model classes including decision trees, neural networks and logistic regression.</p>
<h3 id="membership-inference"><span class="header-section-number">5.4.3</span> Membership inference</h3>
<p>Shokri <span><em>et al.</em></span> <span class="citation" data-cites="7958568">[<a href="#ref-7958568" role="doc-biblioref">52</a>]</span> propose membership inference attacks, in which the adversary’s objective is to determine the model’s training dataset. To do so, the adversary passes examples to an online prediction API and gets its predictions. They use those results to build shadow models designed to replicate the target. Since the training and testing data for their shadow models is known, they build an attack model which compares the target’s predictions for examples that are in the shadow training set to examples that are not. This has privacy implications for sensitive datasets <span><em>e.g.</em></span> in the healthcare sector, where a successful adversary could gather patients’ information. The authors try their attacks on Amazon and Google’s prediction APIs.</p>
<h2 id="adversarial-defenses"><span class="header-section-number">5.5</span> Adversarial defenses</h2>
<p>Protection against adversarial attacks is necessary, though difficult and impractical for two main reasons: first, anticipating adversarial attacks is a difficult task, as their attack vectors and their influences vary wildly; second, devoting too much of the model’s capacity to detecting and stopping attacks could reduce its ability to perform its predictions on non-adversarial examples, which is its primary objective. Furthermore, current defense mechanisms are typically tailored to a specific attack, so complete protection has performance drawbacks and requires longer training. This subsection explores current defense mechanisms.</p>
<h3 id="adversarial-training"><span class="header-section-number">5.5.1</span> Adversarial training</h3>
<p>In adversarial training, the modeler transforms the training data using one of the attacks available to adversaries as a data augmentation technique. In doing so, the learner is forced to recognize these modified examples as belonging to the class of the original version, therefore preventing an adversary from successfully performing the same attack at test time. Shaham <span><em>et al.</em></span> <span class="citation" data-cites="SHAHAM2018195">[<a href="#ref-SHAHAM2018195" role="doc-biblioref">50</a>]</span> present a general framework in which adversarial examples are generated at each update of the learner’s parameters, providing more stable predictions and increasing the difficulty of generating adversarial examples.</p>
<h3 id="defensive-distillation"><span class="header-section-number">5.5.2</span> Defensive Distillation</h3>
<p>Distillation <span class="citation" data-cites="2015arXiv150302531H">[<a href="#ref-2015arXiv150302531H" role="doc-biblioref">18</a>]</span> is a technique for combining an ensemble of machine-learning models into one larger model. It consists of using the probability vectors output by an ensemble of neural networks as training labels for a smaller network (rather than the typical onehot-encoded labels), allowing for reduced complexity of the model and increased prediction speed. Papernot <span><em>et al.</em></span> <span class="citation" data-cites="7546524">[<a href="#ref-7546524" role="doc-biblioref">44</a>]</span> employ distillation as a hardening mechanism for the learner. The idea behind it is that adversarially-altered examples, which closely resemble their original versions, should get similar probability vectors output by the model. Therefore, crafting examples lying on a boundary between classes becomes more difficult as the labels are now softened into real numbers in the [0, 1] range, rather than binaries.</p>
<h3 id="null-labeling"><span class="header-section-number">5.5.3</span> NULL labeling</h3>
<p>While the mechanisms above are effective for adversarial examples generated on the learner model, they do not account for the transferability property of the examples, <span><em>i.e.</em></span> the adversary’s ability to generate examples for another similar model, and transfer them to the target with efficacy. To prevent transferability, Hosseini <span><em>et al.</em></span> <span class="citation" data-cites="2017arXiv170304318H">[<a href="#ref-2017arXiv170304318H" role="doc-biblioref">19</a>]</span> suggest adding an addtional class to the learner’s probability vector. The NULL class, rather than attempting to correctly classify an adversarial example using the same label as the original example, labels attacks as such by simply refusing to classify them. The training is first performed on the original dataset, after which the adversarial examples are assigned a NULL probability, which depends on their amount of corruption. The classifier is finally put through an adversarial training phase with the original and transformed examples, and is expected to assign high NULL probability to the altered inputs.</p>
<h1 id="privacy-conscious-learning"><span class="header-section-number">6</span> Privacy-conscious learning</h1>
<p>While attack prevention is a strong research area, a lot of work is being devoted to avoiding information leakage in the eventuality of an adversarial attack. Privacy-preserving learning deals with data pre-processing and training procedures aimed at containing little-to-no private information about the training data. This is particularly critical in sensitive datasets in the healthcare industry, or confidential business datasets. We present the directions current research is taking towards private learning systems.</p>
<h2 id="noisy-preprocessing"><span class="header-section-number">6.1</span> Noisy preprocessing</h2>
<p>One technique for obfuscating private data is applying noise to it before feeding it to the model.</p>
<h3 id="adding-gaussian-noise"><span class="header-section-number">6.1.1</span> Adding Gaussian noise</h3>
<p>A simple method are for adding noise to the dataset involves linearly transforming the data points. First, Domingo-Ferrer <span><em>et al.</em></span> <span class="citation" data-cites="10.1007/978-3-540-25955-8_12">[<a href="#ref-10.1007/978-3-540-25955-8_12" role="doc-biblioref">9</a>]</span> show how one can add random noise to the inputs <span class="math inline">\(X\)</span> through the following transformation: <span class="math display">\[X^{&#39;} = X + \epsilon, \quad \epsilon \sim N(0, \Sigma_\epsilon = \alpha\Sigma)\]</span> where <span class="math inline">\(\Sigma\)</span> is the covariance matrix of the original dataset and <span class="math inline">\(\alpha\)</span> is ratio of the variance of <span class="math inline">\(X^{&#39;}\)</span> compared to <span class="math inline">\(X\)</span>. This method preserves the mean, as well as the correlations between the input dimensions. Furthermore, it ensures that the covariance matrix of <span class="math inline">\(X^{&#39;}\)</span> is proportional to that of <span class="math inline">\(X\)</span>, since: <span class="math display">\[\Sigma_{X^{&#39;}} = \Sigma + \Sigma_\epsilon = \Sigma + \alpha \Sigma = (1 + \alpha) \cdot \Sigma\]</span></p>
<h3 id="ss:cryptonets"><span class="header-section-number">6.1.2</span> Encrypting the data</h3>
<p>Gilad-Bachrach <span><em>et al.</em></span> <span class="citation" data-cites="gilad2016cryptonets">[<a href="#ref-gilad2016cryptonets" role="doc-biblioref">13</a>]</span> propose CryptoNets, which allow a pretrained model to make predictions based on encrypted data. The training procedure is therefore the same as with regular data, with no additional preprocessing needed. Test data is then encrypted homomorphically in order to maintain its structure. This encryption scheme allows the pretrained neural network to propagate the inputs forward through its activation layers and output a prediction as if it were dealing with the original examples. Their encryption mechanism is efficient enough to output over 59000 predictions per hour on the MNIST dataset of handwritten digits <span class="citation" data-cites="lecun1998mnist">[<a href="#ref-lecun1998mnist" role="doc-biblioref">30</a>]</span> despite the encryption process, but still carries an overhead when compared to training on the plain, unencrypted inputs. This method enables the user to get predictions from pretrained online machine-learning APIs without ever sending their unencrypted data through the Internet.</p>
<h3 id="ss:gc"><span class="header-section-number">6.1.3</span> Using Garbled Circuits</h3>
<p>Rouhani <span><em>et al.</em></span> <span class="citation" data-cites="Rouhani:2018:DSP:3195970.3196023">[<a href="#ref-Rouhani:2018:DSP:3195970.3196023" role="doc-biblioref">49</a>]</span> provide a secure deep learning framework, named <span><em>DeepSecure</em></span>, for making predictions without sharing any information between parties. The authors first perform dimensionality reduction on the original input by projecting it to an ensemble of subspaces. This data is then garbled through the use of Garbled Circuits <span class="citation" data-cites="4568207">[<a href="#ref-4568207" role="doc-biblioref">55</a>]</span> and sent to the server-side learner. The learner evaluates the garbled circuit and returns its prediction based on the encrypted data. The client can then decrypt the prediction using the keys only he has access to, to get the actual predicted label associated to the original input. Figure <a href="#fig:deepsecure" data-reference-type="ref" data-reference="fig:deepsecure">6</a> provides a visual representation of the process.</p>
<figure>
<img src="/assets/images/posts/a-survey-of-game-theoretic-adversarial-learning-and-its-implications-on-privacy/deepsecure.png" id="fig:deepsecure" alt="" /><figcaption>DeepSecure prediction process (Rouhani <span><em>et al.)</em></span><span label="fig:deepsecure"></span></figcaption>
</figure>
<h2 id="ss:federated"><span class="header-section-number">6.2</span> Distributing the training</h2>
<p>Federated learning <span class="citation" data-cites="mcmahan2016communication">[<a href="#ref-mcmahan2016communication" role="doc-biblioref">36</a>]</span> is a novel approach which allows a machine-learning model to be trained without ever accessing user data. Instead of sending their private data, users only send back model-update data to a central cloud server. The process is the following: A model is first initialized by its owner, and optionally pre-trained on a large dataset (if available). The owner then connects to remote devices randomly selected from a pool of devices declaring themselves as eligible for training (McMahan <span><em>el al.</em></span> classify a device as eligible if it is idle and connected to a power source and an unmetered network.) The chosen devices then receive the current model, train it on their local data, and send back only the gradient of the model’s loss function. Once the owner has received a large number of these gradients, it updates its parameters using the average gradient (weighted by each user’s number of training inputs). The updated model is then sent back to users for local use or further training.</p>
<p>This training method effectively removes the need for a relationship between the owner of the model and the owner of the data. However, it suffers from two main drawbacks: first, the training speed will be bottlenecked by each user’s connection to the owner; second, the distribution (and number) of inputs may be very different from one user to another, which could lead the model to be trained on unbalanced or non-representative data.</p>
<h3 id="differential-privacy"><span class="header-section-number">6.2.1</span> Differential privacy</h3>
<p>Differential privacy <span class="citation" data-cites="dwork2011differential">[<a href="#ref-dwork2011differential" role="doc-biblioref">10</a>]</span> is a method used primarily in statistical databases, where it is used as a query-time procedure that provides randomized answers to aggregation queries, that are similar to the answers one would get from the original dataset. The general goal of differential privacy is to prevent someone from telling whether a particular example is present in a dataset. Abadi <span><em>et al.</em></span> <span class="citation" data-cites="Abadi:2016:DLD:2976749.2978318">[<a href="#ref-Abadi:2016:DLD:2976749.2978318" role="doc-biblioref">1</a>]</span> propose a differentially-private extension of the classical SGD algorithm for model training, which applies differential privacy to the learner’s training dataset. In their extension, the training loop has two additional steps: first, the gradient of the loss function is clipped by a factor of <span class="math inline">\(1/C\)</span> multiplied by the the gradient’s norm (where <span class="math inline">\(C\)</span> is a clipping threshold); second, Gaussian noise is added to the clipped gradient. This added noise translates to a small difference in the impact of each training example on the learner’s parameter update. In other words, each parameter update is differentially private with respect to the training examples. Also, at each parameter update, a <span><em>privacy accountant</em></span> accumulates a privacy cost: a score of 0 indicates perfect privacy but may not lead to a high-performance classifier.</p>
<h1 id="discussion---current-limitations"><span class="header-section-number">7</span> Discussion - Current limitations</h1>
<p>Some limitations can be found in the game-theoretic framework, as well as the current performance of adversarial learning. This section presents those limitations.</p>
<h2 id="simplifying-assumptions"><span class="header-section-number">7.1</span> Simplifying assumptions</h2>
<p>First, an assumption being made in the game-theoretical framework is that all players are rational and behaving in their selfish best interest. If that is not the case, the optimal strategy derived from the minimax optimization may lead to suboptimal payoffs in the real world. Second, the underlying assumptions of formalizing learning as a fully-defined game as in <a href="#ss:gt-analysis" data-reference-type="ref" data-reference="ss:gt-analysis">5.2</a> are that the strategy spaces of each player, along with their associated payoffs, are finite and known by both parties. Depending on the definition of the spaces (<span><em>e.g.</em></span> defining a strategy as a set of real-valued parameters rather than models), those assumptions can be easily violated.</p>
<h2 id="finding-and-end-to-the-game"><span class="header-section-number">7.2</span> Finding and end to the game</h2>
<p>It is reasonable to wonder whether the arms race between assailants and defendants has an end. Indeed, progress by either player benefits their opponent as well. For example, new research on successful attack techniques is often followed by research on defensive mechanisms and vice-versa, therefore creating stronger players on both sides and making the game more difficult to solve.</p>
<h2 id="domain-specific-applications"><span class="header-section-number">7.3</span> Domain-specific applications</h2>
<p>Much of the research currently published on adversarial learning focuses on computer vision tasks, such as object recognition and face detection. Other areas such as natural-language processing and automated speech recognition are still lagging behind. Further work into these areas is to be expected as the field matures.</p>
<h2 id="efficient-trust-less-privacy"><span class="header-section-number">7.4</span> Efficient, trust-less privacy</h2>
<p>The privacy-convenience trade-off is one data owners must deal with in the current state of adversarial learning. On the one hand, easy and efficient methods such as adding random noise are always at risk of good-enough adversarial attacks that can recover the original private information. Furthermore, they are based on a relation of trust between the data user and the prediction model, particularly when it is accessible as a black-box oracle: the user must trust that the service does not store his data in insecure ways. On the other hand, trust-less options such as those presented in sections <a href="#ss:cryptonets" data-reference-type="ref" data-reference="ss:cryptonets">6.1.2</a>, <a href="#ss:gc" data-reference-type="ref" data-reference="ss:gc">6.1.3</a> and <a href="#ss:federated" data-reference-type="ref" data-reference="ss:federated">6.2</a> carry a computational overhead.</p>
<h1 id="conclusion"><span class="header-section-number">8</span> Conclusion</h1>
<p>Three main conclusions emerge from our overview of game-theoretic adversarial learning. First, adversarial learning enables a more appropriate view of machine-learning for certain tasks, such as image generation through generative adversarial networks, and cybersecurity. Second, privacy preservation is more prevalent than ever with the advent of cloud-based machine-learning solutions, yet is a problem that has been overlooked until recently. Third and finally, while current results are promising, adversarial learning as well as machine learning still suffer from issues such as training stability, and a full understanding of the learning setting which leads to a difficulty in expanding knowledge to new tasks or providing solid training guarantees. We expect research to continue in the directions of privacy-preserving learning as well as adversarial attacks and defenses.</p>
<h1 id="references" class="unnumbered">References<a class="anchor d-print-none" aria-hidden="true"><i class="fas fa-anchor"></i></a></h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-Abadi:2016:DLD:2976749.2978318">
<p>[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In <em>Proceedings of the 2016 acm sigsac conference on computer and communications security</em> (CCS ’16), 308–318. DOI:<a href="https://doi.org/10.1145/2976749.2978318">https://doi.org/10.1145/2976749.2978318</a></p>
</div>
<div id="ref-2017arXiv170107875A">
<p>[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN. <em>arXiv e-prints</em> (January 2017), arXiv:1701.07875. Retrieved from <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a></p>
</div>
<div id="ref-8547128">
<p>[3] A. J. Bose and P. Aarabi. 2018. Adversarial attacks on face detectors using neural net based constrained optimization. In <em>2018 ieee 20th international workshop on multimedia signal processing (mmsp)</em>, 1–6. DOI:<a href="https://doi.org/10.1109/MMSP.2018.8547128">https://doi.org/10.1109/MMSP.2018.8547128</a></p>
</div>
<div id="ref-NIPS2009_3755">
<p>[4] Michael Brückner and Tobias Scheffer. 2009. Nash equilibria of static prediction games. In <em>Advances in neural information processing systems 22</em>, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams and A. Culotta (eds.). Curran Associates, Inc., 171–179. Retrieved from <a href="http://papers.nips.cc/paper/3755-nash-equilibria-of-static-prediction-games.pdf">http://papers.nips.cc/paper/3755-nash-equilibria-of-static-prediction-games.pdf</a></p>
</div>
<div id="ref-Bruckner:2011:SGA:2020408.2020495">
<p>[5] Michael Brückner and Tobias Scheffer. 2011. Stackelberg games for adversarial prediction problems. In <em>Proceedings of the 17th acm sigkdd international conference on knowledge discovery and data mining</em> (KDD ’11), 547–555. DOI:<a href="https://doi.org/10.1145/2020408.2020495">https://doi.org/10.1145/2020408.2020495</a></p>
</div>
<div id="ref-7958570">
<p>[6] N. Carlini and D. Wagner. 2017. Towards evaluating the robustness of neural networks. In <em>2017 ieee symposium on security and privacy (sp)</em>, 39–57. DOI:<a href="https://doi.org/10.1109/SP.2017.49">https://doi.org/10.1109/SP.2017.49</a></p>
</div>
<div id="ref-2002cs........5074C">
<p>[7] Vincent Conitzer and Tuomas Sandholm. 2002. Complexity Results about Nash Equilibria. <em>arXiv e-prints</em> (May 2002), cs/0205074. Retrieved from <a href="http://arxiv.org/abs/cs/0205074">http://arxiv.org/abs/cs/0205074</a></p>
</div>
<div id="ref-Dekel2010">
<p>[8] Ofer Dekel, Ohad Shamir, and Lin Xiao. 2010. Learning to classify with missing and corrupted features. <em>Machine Learning</em> 81, 2 (November 2010), 149–178. DOI:<a href="https://doi.org/10.1007/s10994-009-5124-8">https://doi.org/10.1007/s10994-009-5124-8</a></p>
</div>
<div id="ref-10.1007/978-3-540-25955-8_12">
<p>[9] Josep Domingo-Ferrer, Francesc Sebé, and Jordi Castellà-Roca. 2004. On the security of noise addition for privacy in statistical databases. In <em>Privacy in statistical databases</em>, 149–161.</p>
</div>
<div id="ref-dwork2011differential">
<p>[10] Cynthia Dwork. 2011. Differential privacy. <em>Encyclopedia of Cryptography and Security</em> (2011), 338–340.</p>
</div>
<div id="ref-eyal2015miner">
<p>[11] Ittay Eyal. 2015. The miner’s dilemma. In <em>2015 ieee symposium on security and privacy</em>, 89–103.</p>
</div>
<div id="ref-Fredrikson:2015:MIA:2810103.2813677">
<p>[12] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information and basic countermeasures. In <em>Proceedings of the 22Nd acm sigsac conference on computer and communications security</em> (CCS ’15), 1322–1333. DOI:<a href="https://doi.org/10.1145/2810103.2813677">https://doi.org/10.1145/2810103.2813677</a></p>
</div>
<div id="ref-gilad2016cryptonets">
<p>[13] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In <em>International conference on machine learning</em>, 201–210.</p>
</div>
<div id="ref-Globerson:2006:NTT:1143844.1143889">
<p>[14] Amir Globerson and Sam Roweis. 2006. Nightmare at test time: Robust learning by feature deletion. In <em>Proceedings of the 23rd international conference on machine learning</em> (ICML ’06), 353–360. DOI:<a href="https://doi.org/10.1145/1143844.1143889">https://doi.org/10.1145/1143844.1143889</a></p>
</div>
<div id="ref-2014arXiv1412.6572G">
<p>[15] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and Harnessing Adversarial Examples. <em>arXiv e-prints</em> (December 2014), arXiv:1412.6572. Retrieved from <a href="http://arxiv.org/abs/1412.6572">http://arxiv.org/abs/1412.6572</a></p>
</div>
<div id="ref-NIPS2014_5423">
<p>[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In <em>Advances in neural information processing systems 27</em>, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence and K. Q. Weinberger (eds.). Curran Associates, Inc., 2672–2680. Retrieved from <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a></p>
</div>
<div id="ref-Hestness:2019:BHA:3293883.3295710">
<p>[17] Joel Hestness, Newsha Ardalani, and Gregory Diamos. 2019. Beyond human-level accuracy: Computational challenges in deep learning. In <em>Proceedings of the 24th symposium on principles and practice of parallel programming</em> (PPoPP ’19), 1–14. DOI:<a href="https://doi.org/10.1145/3293883.3295710">https://doi.org/10.1145/3293883.3295710</a></p>
</div>
<div id="ref-2015arXiv150302531H">
<p>[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. <em>arXiv e-prints</em> (March 2015), arXiv:1503.02531. Retrieved from <a href="http://arxiv.org/abs/1503.02531">http://arxiv.org/abs/1503.02531</a></p>
</div>
<div id="ref-2017arXiv170304318H">
<p>[19] Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. Blocking Transferability of Adversarial Examples in Black-Box Learning Systems. <em>arXiv e-prints</em> (March 2017), arXiv:1703.04318. Retrieved from <a href="http://arxiv.org/abs/1703.04318">http://arxiv.org/abs/1703.04318</a></p>
</div>
<div id="ref-2017arXiv170205983H">
<p>[20] Weiwei Hu and Ying Tan. 2017. Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN. <em>arXiv e-prints</em> (February 2017), arXiv:1702.05983. Retrieved from <a href="http://arxiv.org/abs/1702.05983">http://arxiv.org/abs/1702.05983</a></p>
</div>
<div id="ref-Huang:2011:AML:2046684.2046692">
<p>[21] Ling Huang, Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. 2011. Adversarial machine learning. In <em>Proceedings of the 4th acm workshop on security and artificial intelligence</em> (AISec ’11), 43–58. DOI:<a href="https://doi.org/10.1145/2046684.2046692">https://doi.org/10.1145/2046684.2046692</a></p>
</div>
<div id="ref-2017arXiv170202284H">
<p>[22] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial Attacks on Neural Network Policies. <em>arXiv e-prints</em> (February 2017), arXiv:1702.02284. Retrieved from <a href="http://arxiv.org/abs/1702.02284">http://arxiv.org/abs/1702.02284</a></p>
</div>
<div id="ref-johnson2014game">
<p>[23] Benjamin Johnson, Aron Laszka, Jens Grossklags, Marie Vasek, and Tyler Moore. 2014. Game-theoretic analysis of ddos attacks against bitcoin mining pools. In <em>International conference on financial cryptography and data security</em>, 72–86.</p>
</div>
<div id="ref-2017arXiv171010196K">
<p>[24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive Growing of GANs for Improved Quality, Stability, and Variation. <em>arXiv e-prints</em> (October 2017), arXiv:1710.10196. Retrieved from <a href="http://arxiv.org/abs/1710.10196">http://arxiv.org/abs/1710.10196</a></p>
</div>
<div id="ref-kearns2008graphical">
<p>[25] Michael Kearns. 2008. Graphical games. <em>The New Palgrave Dictionary of Economics: Volume 1–8</em> (2008), 2547–2549.</p>
</div>
<div id="ref-pmlr-v9-kloft10a">
<p>[26] Marius Kloft and Pavel Laskov. 2010. Online anomaly detection under adversarial impact. In <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em> (Proceedings of machine learning research), 405–412. Retrieved from <a href="http://proceedings.mlr.press/v9/kloft10a.html">http://proceedings.mlr.press/v9/kloft10a.html</a></p>
</div>
<div id="ref-Koutsoupias:1999:WE:1764891.1764944">
<p>[27] Elias Koutsoupias and Christos Papadimitriou. 1999. Worst-case equilibria. In <em>Proceedings of the 16th annual conference on theoretical aspects of computer science</em> (STACS’99), 404–413. Retrieved from <a href="http://dl.acm.org/citation.cfm?id=1764891.1764944">http://dl.acm.org/citation.cfm?id=1764891.1764944</a></p>
</div>
<div id="ref-2016arXiv160702533K">
<p>[28] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world. <em>arXiv e-prints</em> (July 2016), arXiv:1607.02533. Retrieved from <a href="http://arxiv.org/abs/1607.02533">http://arxiv.org/abs/1607.02533</a></p>
</div>
<div id="ref-2016arXiv161101236K">
<p>[29] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial Machine Learning at Scale. <em>arXiv e-prints</em> (November 2016), arXiv:1611.01236. Retrieved from <a href="http://arxiv.org/abs/1611.01236">http://arxiv.org/abs/1611.01236</a></p>
</div>
<div id="ref-lecun1998mnist">
<p>[30] Yann LeCun. 1998. The mnist database of handwritten digits. <em>http://yann. lecun. com/exdb/mnist/</em> (1998).</p>
</div>
<div id="ref-lecun1995convolutional">
<p>[31] Yann LeCun, Yoshua Bengio, and others. 1995. Convolutional networks for images, speech, and time series. <em>The handbook of brain theory and neural networks</em> 3361, 10 (1995), 1995.</p>
</div>
<div id="ref-Ledig_2017_CVPR">
<p>[32] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. 2017. Photo-realistic single image super-resolution using a generative adversarial network. In <em>The ieee conference on computer vision and pattern recognition (cvpr)</em>.</p>
</div>
<div id="ref-NIPS2016_6142">
<p>[33] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poisoning attacks on factorization-based collaborative filtering. In <em>Advances in neural information processing systems 29</em>, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon and R. Garnett (eds.). Curran Associates, Inc., 1885–1893. Retrieved from <a href="http://papers.nips.cc/paper/6142-data-poisoning-attacks-on-factorization-based-collaborative-filtering.pdf">http://papers.nips.cc/paper/6142-data-poisoning-attacks-on-factorization-based-collaborative-filtering.pdf</a></p>
</div>
<div id="ref-Liu2010">
<p>[34] Wei Liu and Sanjay Chawla. 2010. Mining adversarial patterns via regularized loss minimization. <em>Machine Learning</em> 81, 1 (October 2010), 69–83. DOI:<a href="https://doi.org/10.1007/s10994-010-5199-2">https://doi.org/10.1007/s10994-010-5199-2</a></p>
</div>
<div id="ref-luu2015power">
<p>[35] Loi Luu, Ratul Saha, Inian Parameshwaran, Prateek Saxena, and Aquinas Hobor. 2015. On power splitting games in distributed computation: The case of bitcoin pooled mining. In <em>2015 ieee 28th computer security foundations symposium</em>, 397–411.</p>
</div>
<div id="ref-mcmahan2016communication">
<p>[36] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and others. 2016. Communication-efficient learning of deep networks from decentralized data. <em>arXiv preprint arXiv:1602.05629</em> (2016).</p>
</div>
<div id="ref-Moosavi-Dezfooli_2016_CVPR">
<p>[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: A simple and accurate method to fool deep neural networks. In <em>The ieee conference on computer vision and pattern recognition (cvpr)</em>.</p>
</div>
<div id="ref-myerson1997game">
<p>[38] R. B. Myerson. 1997. <em>Game theory: Analysis of conflict</em>. Harvard University Press. Retrieved from <a href="https://books.google.ca/books?id=E8WQFRCsNr0C">https://books.google.ca/books?id=E8WQFRCsNr0C</a></p>
</div>
<div id="ref-nakamoto2008bitcoin">
<p>[39] Satoshi Nakamoto and others. 2008. Bitcoin: A peer-to-peer electronic cash system. (2008).</p>
</div>
<div id="ref-nash1951non">
<p>[40] John Nash. 1951. Non-cooperative games. <em>Annals of mathematics</em> (1951), 286–295.</p>
</div>
<div id="ref-neumann1928theorie">
<p>[41] J v Neumann. 1928. Zur theorie der gesellschaftsspiele. <em>Mathematische annalen</em> 100, 1 (1928), 295–320.</p>
</div>
<div id="ref-8237427">
<p>[42] S. J. Oh, M. Fritz, and B. Schiele. 2017. Adversarial image perturbation for privacy protection a game theory perspective. In <em>2017 ieee international conference on computer vision (iccv)</em>, 1491–1500. DOI:<a href="https://doi.org/10.1109/ICCV.2017.165">https://doi.org/10.1109/ICCV.2017.165</a></p>
</div>
<div id="ref-7467366">
<p>[43] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. 2016. The limitations of deep learning in adversarial settings. In <em>2016 ieee european symposium on security and privacy (euros p)</em>, 372–387. DOI:<a href="https://doi.org/10.1109/EuroSP.2016.36">https://doi.org/10.1109/EuroSP.2016.36</a></p>
</div>
<div id="ref-7546524">
<p>[44] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In <em>2016 ieee symposium on security and privacy (sp)</em>, 582–597. DOI:<a href="https://doi.org/10.1109/SP.2016.41">https://doi.org/10.1109/SP.2016.41</a></p>
</div>
<div id="ref-Park_2019_CVPR">
<p>[45] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In <em>The ieee conference on computer vision and pattern recognition (cvpr)</em>.</p>
</div>
<div id="ref-2015arXiv151106434R">
<p>[46] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. <em>arXiv e-prints</em> (November 2015), arXiv:1511.06434. Retrieved from <a href="http://arxiv.org/abs/1511.06434">http://arxiv.org/abs/1511.06434</a></p>
</div>
<div id="ref-NIPS2015_5638">
<p>[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In <em>Advances in neural information processing systems 28</em>, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett (eds.). Curran Associates, Inc., 91–99. Retrieved from <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</a></p>
</div>
<div id="ref-rosenblatt1958perceptron">
<p>[48] Frank Rosenblatt. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological review</em> 65, 6 (1958), 386.</p>
</div>
<div id="ref-Rouhani:2018:DSP:3195970.3196023">
<p>[49] Bita Darvish Rouhani, M. Sadegh Riazi, and Farinaz Koushanfar. 2018. Deepsecure: Scalable provably-secure deep learning. In <em>Proceedings of the 55th annual design automation conference</em> (DAC ’18), 2:1–2:6. DOI:<a href="https://doi.org/10.1145/3195970.3196023">https://doi.org/10.1145/3195970.3196023</a></p>
</div>
<div id="ref-SHAHAM2018195">
<p>[50] Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2018. Understanding adversarial training: Increasing local stability of supervised models through robust optimization. <em>Neurocomputing</em> 307, (2018), 195–204. DOI:<a href="https://doi.org/https://doi.org/10.1016/j.neucom.2018.04.027">https://doi.org/https://doi.org/10.1016/j.neucom.2018.04.027</a></p>
</div>
<div id="ref-SHEN20143">
<p>[51] Yue Shen, Zheng Yan, and Raimo Kantola. 2014. Analysis on the acceptance of global trust management for unwanted traffic control based on game theory. <em>Computers &amp; Security</em> 47, (2014), 3–25. DOI:<a href="https://doi.org/https://doi.org/10.1016/j.cose.2014.03.010">https://doi.org/https://doi.org/10.1016/j.cose.2014.03.010</a></p>
</div>
<div id="ref-7958568">
<p>[52] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership inference attacks against machine learning models. In <em>2017 ieee symposium on security and privacy (sp)</em>, 3–18. DOI:<a href="https://doi.org/10.1109/SP.2017.41">https://doi.org/10.1109/SP.2017.41</a></p>
</div>
<div id="ref-2017arXiv170403453T">
<p>[53] Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. The Space of Transferable Adversarial Examples. <em>arXiv e-prints</em> (April 2017), arXiv:1704.03453. Retrieved from <a href="http://arxiv.org/abs/1704.03453">http://arxiv.org/abs/1704.03453</a></p>
</div>
<div id="ref-197128">
<p>[54] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing machine learning models via prediction apis. In <em>25th USENIX security symposium (USENIX security 16)</em>, 601–618. Retrieved from <a href="https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer">https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer</a></p>
</div>
<div id="ref-4568207">
<p>[55] A. C. Yao. 1986. How to generate and exchange secrets. In <em>27th annual symposium on foundations of computer science (sfcs 1986)</em>, 162–167. DOI:<a href="https://doi.org/10.1109/SFCS.1986.25">https://doi.org/10.1109/SFCS.1986.25</a></p>
</div>
<div id="ref-10.1007/978-3-319-31750-2_28">
<p>[56] Yan Zhou and Murat Kantarcioglu. 2016. Modeling adversarial learning as nested stackelberg games. In <em>Advances in knowledge discovery and data mining</em>, 350–362.</p>
</div>
<div id="ref-Zhou:2012:ASV:2339530.2339697">
<p>[57] Yan Zhou, Murat Kantarcioglu, Bhavani Thuraisingham, and Bowei Xi. 2012. Adversarial support vector machine learning. In <em>Proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining</em> (KDD ’12), 1059–1067. DOI:<a href="https://doi.org/10.1145/2339530.2339697">https://doi.org/10.1145/2339530.2339697</a></p>
</div>
<div id="ref-CycleGAN2017">
<p>[58] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. In <em>Computer vision (iccv), 2017 ieee international conference on</em>.</p>
</div>
</div>
